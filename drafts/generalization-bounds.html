<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.dropotron.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/skel-layers.min.js"></script>
    <script src="/js/init.js"></script>
    <link rel="stylesheet" href="/css/pygment.css" />
    <noscript>
        <link rel="stylesheet" href="/css/skel.css" />
        <link rel="stylesheet" href="/css/style.css" />
        <link rel="stylesheet" href="/css/style-noscript.css" />
    </noscript>
    <script src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>
    <!-- <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" integrity="sha384-3AB7yXWz4OeoZcPbieVW64vVXEwADiYyAEhwilzWsLw+9FgqpyjjStpPnpBO8o8S" crossorigin="anonymous">
    <link  href="http://fonts.googleapis.com/css?family=Anonymous+Pro:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css" >
    <!--[if lte IE 8]><link rel="stylesheet" href="/css/ie/v8.css" /><![endif]-->
    <!--[if lte IE 9]><link rel="stylesheet" href="/css/ie/v9.css" /><![endif]-->

    <title>Statistical Learning Theory and Generalization Bounds | &#12296&nbsp;physics&nbsp;&#124;&nbsp;machine&nbsp;learning&nbsp;&#12297; </title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width">
</head>

<body class=" loading">



    <!-- Header -->
    <header id="header" >
        <h1 class="logo">
            <a href="..">&#12296&nbsp;physics&nbsp;&#124;&nbsp;machine&nbsp;learning&nbsp;&#12297;</a>
        </h1>
        <nav id="nav">
            <ul>
                <!-- <li class="current"><a href="index.html">Welcome</a></li> -->
                    <li><a href="/category/news.html">News</a></li>
                    <li><a href="/category/articles.html">Blog</a></li>
                    <li><a href="/pages/papers.html">Papers</a></li>
                <!--
                <li class="submenu">
                    <a href="../">No Blog</a>
                    <ul>
                            <li class="active">
                                <a href="../category/articles/">Articles</a>
                            </li>
                            <li >
                                <a href="../category/news/">News</a>
                            </li>
                    </ul>
                </li>
                -->
                <!--
                <li><a href="#" class="button special">Nothing to Sign Up to</a></li>
                -->
            </ul>
        </nav>
    </header>

<!-- Main -->
<article id="main">

    <header class="special container">
        <!-- <span class="icon fa-"></span> -->
        <h2>Statistical Learning Theory and Generalization Bounds</h2>
        <!-- add page sub title here -->
        <p>Posted in
            <a href="../category/articles.html">Articles</a> on 23-04-2018 by Anna Go</p>
        <!-- <p></p> -->
    </header>

    <style>
        .bordered {
            width: 200px;
            height: 100px;
            padding: 20px;
            border: 1px solid darkorange;
            border-radius: 8px;
        }

        .tagcloud {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
        }

        .tagcloud a {
            display: inline-block;
            font-size: 0.9em;
            margin: 0.125rem;
            padding: 0.4375rem;
            background: #f3f6fa;
            border: 0px solid rgba(0, 0, 255, 0.2);
            border-radius: 40px;
            transition: all 0.1s ease-in-out;
        }

        .tagcloud a:hover,
        .tagcloud a:focus {
            background: grey;
            color: white;
            transform: scale(1.1);
        }
    </style>


    <!-- One -->
    <section class="wrapper style4 container">

        <!-- Content -->
        <div class="content">
            <section>
                <!-- <a href="#" class="image feature"><img src="images/pic04.jpg" alt="" /></a> -->
                <!-- <h3>Posted in <a href="../category/articles.html">Articles</a></h3> -->
                <p><p>When setting up a supervised learning task, we define an ansatz for the 
target function <span class="math">\(f\)</span> by making an (implicit) assumption about the form of 
<span class="math">\(f\)</span>. This assumption defines the space of possible functions, which is 
called the <strong>hypothesis space</strong> <span class="math">\(\mathcal{H}\)</span>. For example, if we assume that 
the target function is linear, then <span class="math">\(\mathcal{H}\)</span> can be formally expressed 
as</p>
<div class="math">$$
\mathcal{H} = \{ h: \mathcal{X}\to\mathcal{Y} \,|\,Â h(x) = ax+b \},
$$</div>
<p>that is the set of all linear functions <span class="math">\(h\)</span> mapping from the input space 
<span class="math">\(\mathcal{X}\)</span> to the output space <span class="math">\(\mathcal{Y}\)</span>.</p>
<p>The task of the machine learning algorithm is now to choose from 
<span class="math">\(\mathcal{H}\)</span> a single hypothesis <span class="math">\(h\)</span> which provides the best estimate 
for the target function <span class="math">\(f\)</span>. The quality of the estimate is measured 
by the <strong>loss</strong> or <strong>cost function</strong> <span class="math">\(L(y,h(x))\)</span>, which measures the discrepancy 
between the true label <span class="math">\(y\)</span> of <span class="math">\(x\)</span> and its prediction <span class="math">\(h(x)\)</span>. The performance 
of a hypothesis <span class="math">\(h\)</span> on the full dataset is evaluated by computing the mean 
loss,</p>
<div class="math">$$
R_{emp}(h) = \frac{1}{m}\sum\limits_{i=1}^{m} L(y_i,h(x_i))
$$</div>
<p>referred to as the <strong>empirical risk</strong>, while the expected value of the loss 
over the true unknown joint probability distribution <span class="math">\(P(X,Y)\)</span> is the
(true) risk:</p>
<div class="math">$$
R(h) = \mathbb{E}_{(x,y)\sim P(X,Y)} \left[ L(y,h(x)) \right]
$$</div>
<p>However, minimizing the empirical risk alone is not sufficient; the 
<strong>generalization gap</strong> (GG) - that is the difference between the risk and 
empirical risk - has to be small. Formally, we require that for some small 
positive number <span class="math">\(\varepsilon\)</span> the following probability is small:</p>
<div class="math">$$
P\left[ \sup \limits_{h\in\mathcal{H}} \left| R(h)-R_{emp}(h) \right| \gt \varepsilon \right]
$$</div>
<p>Consider that an important underlying assumption of the learning task setting is that 
the samples in the training set are independent and identically distributed (iid), 
that is drawn from the same probability distribution. Given this assumption, we can 
make use of the</p>
<h2>Law of Large Numbers</h2>
<blockquote>
<p>If <span class="math">\(x_1,...,x_m\)</span> are <span class="math">\(m\)</span> iid samples of a random variable <span class="math">\(X\)</span> 
  distributed according to <span class="math">\(P\)</span>, then for a small <span class="math">\(\varepsilon\gt 0\)</span>:
  <div class="math">$$
  \lim\limits_{m\to\infty} \mathbb{P}\left[ \left| \mathbb{E}_{X\sim P}[X] - \frac{1}{m}\sum\limits_{i=1}^{m}x_i \right| \gt \varepsilon \right] = 0
  $$</div>
</p>
</blockquote>
<p>The above is the weak formulation of the Law of Large Numbers, stating 
that when the number of samples goes to infinity, the empirical mean 
over the sample will be (almost) equal to the true expectation value of 
<span class="math">\(X\)</span>.</p>
<p>Now, the empirical risk <span class="math">\(R_{emp}\)</span> is the sample mean of the errors, 
while the risk <span class="math">\(R\)</span> is the true mean. Therefore, for a single hypothesis 
we can state:</p>
<div class="math">$$
\lim\limits_{m\to\infty} \mathbb{P}\left[ \left| R(h)-R_{emp}(h) \right| \gt \varepsilon \right] = 0
$$</div>
<h2>Hoeffding's inequality</h2>
<p>quantifies the deviation between random variables and their expected values:</p>
<blockquote>
<p>If <span class="math">\(x_1,...,x_m\)</span> are <span class="math">\(m\)</span> iid samples of a random variable <span class="math">\(X\)</span> 
  distributed according to <span class="math">\(P\)</span>, and <span class="math">\(a \leq x_i \leq b\)</span> for every <span class="math">\(i\)</span>,
  then for a small <span class="math">\(\varepsilon\gt 0\)</span>:
  <div class="math">$$
  \mathbb{P}\left[ \left| \mathbb{E}_{X\sim P}[X] - \frac{1}{m}\sum\limits_{i=1}^{m}x_i \right| \gt \varepsilon \right] \leq 2 \exp\left[ \frac{-2m\varepsilon^2}{(b-a)^2} \right].
  $$</div>
</p>
</blockquote>
<p>Applied to the error (which is normalized, i.e. <span class="math">\(0\leq \varepsilon\leq 1\)</span>), 
we get the following result for a single hypothesis:</p>
<div class="math">$$
\mathbb{P}\left[ \left| R(h)-R_{emp}(h) \right| \gt \varepsilon \right] \leq 2 e^{-2m\varepsilon^2},
$$</div>
<p>showing that the GG decreases exponentially in the number of training examples <span class="math">\(m\)</span>.</p>
<p>However, the GG-bound has to take into account the challenge of picking 
the "right" hypothesis <span class="math">\(h\)</span> from the hypothesis space <span class="math">\(\mathcal{H}\)</span>!</p>
<p>Formally, this can be expressed as
</p>
<div class="math">$$
\mathbb{P}\left[ \sup_{h\in\mathcal{H}}\left| R(h)-R_{emp}(h) \right| \gt \varepsilon \right] = \mathbb{P}\left[ \bigcup\limits_{h\in\mathcal{H}}\left| R(h)-R_{emp}(h) \right| \gt \varepsilon \right]
$$</div>
<p>where <span class="math">\(\bigcup\)</span> denotes the union of events, corresponding to the logical OR operator.</p>
<h2>The Union Bound</h2>
<p>or Bool's inequality states that for any finite countable set of events, 
the probability that at least one of the events happens is no greater than 
the sum of the probability of the individual events. Formally,</p>
<blockquote>
<p>For a countable set of events <span class="math">\(A_i\)</span>, <span class="math">\(i=1,2,...\)</span>,
  <div class="math">$$
  \mathbb{P}\left[ \bigcup\limits_{i} A_i \right] \leq \sum\limits_i \mathbb{P}\left(A_i \right).
  $$</div>
</p>
</blockquote>
<p>Applying this, we obtain</p>
<div class="math">$$
\begin{aligned}
\mathbb{P}\left[ \sup_{h\in\mathcal{H}}\left| R(h)-R_{emp}(h) \right| \gt \varepsilon \right] &amp;\leq \sum\limits_{h\in\mathcal{H}} \mathbb{P}\left[ left| R(h)-R_{emp}(h) \right| \gt \varepsilon \right]\\
&amp;\leq 2|\mathcal{H}| e^{-2m\varepsilon^2},
\end{aligned}
$$</div>
<p>where <span class="math">\(|\mathcal{H}|\)</span> is the cardinality of the hypothesis space.</p>
<p>Denoting the last expression on the right-hand side as <span class="math">\(\delta\)</span>, 
we can state that with a confidence <span class="math">\(1-\delta\)</span>, </p>
<div class="math">$$
\left| R(h)-R_{emp}(h) \right| \leq \varepsilon
$$</div>
<p>or equivalently,
</p>
<div class="math">$$
R(h) \leq R_{emp}(h) + \varepsilon
$$</div>
<p>where
</p>
<div class="math">$$
\varepsilon = \sqrt{ \frac{\ln \left|\mathcal{H}\right| + \ln(2/\delta)}{2m} }.
$$</div>
<p>This is a typical result for a <em>generalization bound</em> studied extensively 
in statistical learning theory. Various modification of this expression 
exist, depending on the details of the derivation assumptions.</p>
<p>The <em>sample complexity</em> of a machine learning algorithm represents the number of training-samples that it needs in order to successfully learn a target function.</p>
<p><span class="math">\(\abs{\mathcal{H}}\)</span> is the cardinality of the hypothesis space, i.e. the number of different functions in the considered function class. In case when <span class="math">\(\abs{\mathcal{H}}=\infty\)</span>, the epsilon-cover of the class is used, i.e. the class is discretized by clustering hypotheses that are <span class="math">\(\epsilon\)</span>-close to each other, such that</p>
<div class="math">$$
\abs{ \mathcal{H}_{\epsilon} } = \left(\frac{1}{\epsilon}\right)^d,
$$</div>
<p>
where <span class="math">\(d\)</span> is the class dimension, typically the VC dimension.</p>
<p>Aside:</p>
<h2>The VC dimension</h2>
<p>The Vapnik-Chervonenkis (VC) dimension is a measure for the capacity of a model, i.e. a class of functions <span class="math">\(\{f(\alpha)\}\)</span>.</p>
<p>R(x)-R_emp(x) is bounded by the expression which is a funciton of VC dim.
Given several different learning machines, we pick the one which minimizes R_emp and the sqrt, i.e. we choose the machine which gives the lowest upper bound on the actual risk. This gives a principled method for choosing a learning machine for a given taskt, and is the essential idea of structured risk minimization.</p>
<p>The VC dim is a property of a set of functions {f(alpha)} and can be defined for various classes of funcitons.</p>
<p>Consider the binary classification task, so that f(alpha,x)\in{-1,1}.</p>
<p>If a given set of m points can be labeled in all possible 2^m ways, and for each labeling, a member of the set {f(alpha)} can be found which correctly assigns those labels, we say that the set of points is hattered by that set of functions. The VC dim for the set of functions {f(alpha)} is defined as the maximum number of training points that can be shattered by {f(alpha)}.</p>
<p>Note that if the VC dim is h, then there exists at least one set of h points that can be shattered, but in general it will not be true that every set of h points can be shattered!</p>
<p>Example: shattering points with oriented hyperplanes in R^n</p>
<p>Three shattered points in the plane - figure</p>
<p>In general, for linear classifiers in R^n, VCdim(\mathcal{H})=n+1.</p>
<p>EXPLAIN HOW VC DIM ESTIMATES THE HYPOTHESIS SPACE CARDINALITY!!!</p>
<p>This bound is not applicable in case of Deep Learning:
The higher expressivity of a DNN would increase the bound, 
that is, it would imply worse generalization. However, it 
is empirically known, that higher expressivity typically 
leads to better generalization results.
Thus, the hypothesis space is not a useful notion in DL.</p>
<p>Consider the input space <span class="math">\(\mathcal{X}\)</span> instead: </p>
<p>The cardinality <span class="math">\(|\mathcal{X}|\)</span> is the number of samples, or data points 
<span class="math">\(x\in\mathcal{X}\)</span>, and each <span class="math">\(x\)</span> has an assigned binary label 
<span class="math">\(y\in\mathcal{Y}=\{0,1\}\)</span>. On total, there are <span class="math">\(2^{|\mathcal{X}|}\)</span> possible 
labelings of the input, only one of which is <em>true</em> - and the goal of the 
machine learning task is to pick the true labeling from the entire set of 
possibilities.</p>
<p>Essentially, the classification task amounts to partitioning the input space 
in clusters corresponding to the labels - in case of binary classification, 
there are two. The clusters evolve gradually in the process of learning.</p>
<p>The input space <span class="math">\(\mathcal{X}\)</span> is divided into <span class="math">\(\epsilon\)</span>-spheres, denoted as 
<span class="math">\(T_{\epsilon}\)</span>, meaning that the probability of two data points from the same 
sphere having different labels is less than <span class="math">\(\epsilon\)</span>. This homogeneity of the 
clusters <span class="math">\(T_{\epsilon}\)</span> is achieved through some constraint, which can be expressed 
in the IB formalism as <span class="math">\(\langle d_{IB} \rangle = I(X;Y)-I(T;Y)\)</span>. Therefore, minimizing
<span class="math">\(\langle d_{IB} \rangle\)</span> amounts to maximizing <span class="math">\(I(T;Y)\)</span>.</p>
<p>Now some information theory: Consider the mapping from <span class="math">\(\mathcal{X}\)</span> to <span class="math">\(\mathcal{T}\)</span> as 
encoding the input samples <span class="math">\(x\)</span> as binary sequences of length <span class="math">\(n\)</span>.
The total number of (typical) <span class="math">\(n\)</span>-sequences for <span class="math">\(X\)</span> is <span class="math">\(2^{nH(X)}\)</span>, and for each typical <span class="math">\(n\)</span>-sequence of <span class="math">\(T\)</span>-symbols there are <span class="math">\(2^{nH(X|T)}\)</span> possible <span class="math">\(n\)</span>-sequences of <span class="math">\(X\)</span>.</p>
<p>The number of <span class="math">\(n\)</span>-sequences that can be transmitted reliably from <span class="math">\(X\)</span> to <span class="math">\(T\)</span> is
<span class="math">\(|\mathcal{T}_{\epsilon}| \sim 2^{H(X)/H(X|T_{\epsilon}})} = 2^{I(T_{\epsilon};X)}\)</span></p>
<p>The number of possible 
function is exponential in the size of this cover, i.e.</p>
<div class="math">$$
\left|\mathcal{H}_{\epsilon}\right| \sim 2^{T_{\epsilon}}
$$</div>
<p>The essential conclusion: the effect of a compression of the input by <span class="math">\(K\)</span> bits is equivalent to the effect of increasing the number of training examples by a factor of <span class="math">\(2^K\)</span>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></p>
                <br />
                <br />
                <p class="tags tagcloud">
                     <a href="../tag/learning-theory/">learning theory</a>
                    <a href="../tag/machine-learning/">machine learning</a>
                    <a href="../tag/supervised-learning/">supervised learning</a>
                 </p>
            </section>
        </div>

    </section>

    <!-- Two 
    <section class="wrapper style1 container special">
        <div class="row">
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/conference-on-machine-learning-and-physics-at-iastu.html" rel='bookmark'><h3>Conference on Machine Learning and Physics at IASTU</h3></a>
              </header>
              <p>The Insitute for Advanced Study of Tsinghua University in Beijing invites to the first of the biannual conference series <a href="http://mlphys2018.csp.escienc</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/conference-on-machine-learning-and-physics-at-iastu.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/program-on-machine-learning-for-quantum-many-body-physics-at-kitp.html" rel='bookmark'><h3>Program on Machine Learning for Quantum Many-Body Physics at KITP</h3></a>
              </header>
              <p>The Kavli Institute for Theoretical Physics in Santa Barbara announces the program "Machine Learning f</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/program-on-machine-learning-for-quantum-many-body-physics-at-kitp.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/workshop-machine-learning-for-quantum-many-body-physics-at-mpipks.html" rel='bookmark'><h3>Workshop Machine Learning for Quantum Many-Body Physics at mpipks</h3></a>
              </header>
              <p>The Max Planck Institute for the Physics of Complex Systems in Dresden hosts "Machine Learning for Quantum Many-Body Ph</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/workshop-machine-learning-for-quantum-many-body-physics-at-mpipks.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
        </div>
    </section> -->

</article>

 
<!-- Footer -->
<footer id="footer">

    <ul class="icons">
    </ul>

    <span class="copyright">&copy; physicsml. All rights reserved. <br> Powered by <a href="https://blog.getpelican.com">Pelican</a>. Theme Twenty, Design: <a href="http://html5up.net">HTML5 UP</a>. Implemented and maintained by Anna Go.</span>

</footer>
</body>
</html>