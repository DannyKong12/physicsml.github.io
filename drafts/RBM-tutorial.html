<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <!--[if lte IE 8]><script src="css/ie/html5shiv.js"></script><![endif]-->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.dropotron.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/skel-layers.min.js"></script>
    <script src="/js/init.js"></script>
    <link rel="stylesheet" href="/css/pygment.css" />
    <noscript>
        <link rel="stylesheet" href="/css/skel.css" />
        <link rel="stylesheet" href="/css/style.css" />
        <link rel="stylesheet" href="/css/style-noscript.css" />
    </noscript>
    <script src="//cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>
    <!-- <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"> -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.8/css/all.css" integrity="sha384-3AB7yXWz4OeoZcPbieVW64vVXEwADiYyAEhwilzWsLw+9FgqpyjjStpPnpBO8o8S" crossorigin="anonymous">
    <link  href="http://fonts.googleapis.com/css?family=Anonymous+Pro:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css" >
    <!--[if lte IE 8]><link rel="stylesheet" href="/css/ie/v8.css" /><![endif]-->
    <!--[if lte IE 9]><link rel="stylesheet" href="/css/ie/v9.css" /><![endif]-->

    <title>How to build a Restricted Boltzmann Machine | &#12296&nbsp;physics&nbsp;&#124;&nbsp;machine&nbsp;learning&nbsp;&#12297; </title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width">
</head>

<body class=" loading">



    <!-- Header -->
    <header id="header" >
        <h1 class="logo">
            <a href="..">&#12296&nbsp;physics&nbsp;&#124;&nbsp;machine&nbsp;learning&nbsp;&#12297;</a>
        </h1>
        <nav id="nav">
            <ul>
                <!-- <li class="current"><a href="index.html">Welcome</a></li> -->
                    <li><a href="/news.html">News</a></li>
                    <li><a href="/articles.html">Blog</a></li>
                    <li><a href="/papers.html">Papers</a></li>
                <!--
                <li class="submenu">
                    <a href="../">No Blog</a>
                    <ul>
                            <li class="active">
                                <a href="../category/articles/">Articles</a>
                            </li>
                            <li >
                                <a href="../category/news/">News</a>
                            </li>
                    </ul>
                </li>
                -->
                <!--
                <li><a href="#" class="button special">Nothing to Sign Up to</a></li>
                -->
            </ul>
        </nav>
    </header>

<!-- Main -->
<article id="main">

    <header class="special container">
        <!-- <span class="icon fa-"></span> -->
        <h2>How to build a Restricted Boltzmann Machine</h2>
        <!-- add page sub title here -->
        <p>Posted in
            <a href="../category/articles.html">Articles</a> on 22-05-2018 by Anna Go</p>
        <!-- <p></p> -->
    </header>

    <style>
        .bordered {
            width: 200px;
            height: 100px;
            padding: 20px;
            border: 1px solid darkorange;
            border-radius: 8px;
        }

        .tagcloud {
            display: flex;
            flex-wrap: wrap;
            align-items: center;
        }

        .tagcloud a {
            display: inline-block;
            font-size: 0.9em;
            margin: 0.125rem;
            padding: 0.4375rem;
            background: #f3f6fa;
            border: 0px solid rgba(0, 0, 255, 0.2);
            border-radius: 40px;
            transition: all 0.1s ease-in-out;
        }

        .tagcloud a:hover,
        .tagcloud a:focus {
            background: grey;
            color: white;
            transform: scale(1.1);
        }
    </style>


    <!-- One -->
    <section class="wrapper style4 container">

        <!-- Content -->
        <div class="content">
            <section>
                <!-- <a href="#" class="image feature"><img src="images/pic04.jpg" alt="" /></a> -->
                <!-- <h3>Posted in <a href="../category/articles.html">Articles</a></h3> -->
                <p><h1>RBMs</h1>
<h2>Sources:</h2>
<ul>
<li>Hinton: practical guide</li>
<li>Fischer Igel: tutorial</li>
<li>Larochelle: lecture notes and videos</li>
<li>Marlin: Inductive Principles for RBM Learning</li>
</ul>
<p>This article is meant as a practical tutorial provising basic knowledge and guidance to build an RBM.</p>
<h2>General Motivation About RBMs</h2>
<p>Restricted Boltzmann Machines (RBM) are top method for unsupervised learning. In the unsupervised setting, only inputs <span class="math">\(x\)</span> are given - there are no labels.
The use of unsupervised learning can be:
- automatically extract meaningful features for your data
- leverage the availability of unlabeled data
- add a data-dependent regularizer to training</p>
<p>A RBM is a two-layer undirected graphical model
the first layer consists of observed data variables (or visible units)
the second layer consists of latent variables (hidden units)
The visible layer is fully connected to the hidden layer via pair-wise potentials, while both the visible and hidden layers are restricted to have no within-layer connections</p>
<p>Define <span class="math">\(D\)</span> to be the number of data dimensions and <span class="math">\(K\)</span> to be the number of hidden units. The space of visible vectors for a binary RBM is <span class="math">\(\mathcal{X}=\{0,1\}^D\)</span>, while the space of hidden unit vectors is <span class="math">\(\mathcal{H}=\{0,1\}^K\)</span>. Define <span class="math">\(x\)</span> to be a visible vector of size <span class="math">\(D\times 1\)</span> and <span class="math">\(h\)</span> to be hidden vector of size <span class="math">\(K\times 1\)</span>.</p>
<p>The RBM model can be defined in terms of the energy function of a joint configuration of the hidden and visible vectors, <span class="math">\(E(x,h)\)</span>, where <span class="math">\(W\)</span> are the weight parameters, <span class="math">\(b\)</span>/<span class="math">\(c\)</span> are the visible/hidden unit bias parameters. We represent <span class="math">\(W\)</span> as a <span class="math">\(D\times K\)</span> matrix, <span class="math">\(b\)</span> as <span class="math">\(D\times 1\)</span> vector and <span class="math">\(c\)</span> as <span class="math">\(K\times 1\)</span> vector. We define <span class="math">\(\theta = \{W,b,c\}\)</span> to be the complete set of parameters for the model. Note that we follow the convention that configurations with high probability have low energy.</p>
<p>The joint probability <span class="math">\(P_{\theta}(v,h)\)</span> can in turn be defined through the energy.</p>
<p>The partition function involves a sum over all <span class="math">\(s^D\)</span> elements of the set <span class="math">\(\mathcal{X}\)</span>, as well as the <span class="math">\(2^K\)</span> elements of the set <span class="math">\(\mathcal{H}\)</span>. The probability of the visible vector <span class="math">\(P_{\theta}(v)\)</span> is obtained by marginalizing over the space <span class="math">\(\mathcal{H}\)</span> of hidden vectors.</p>
<p>An important property of binary hidden unit RBMs is that marginalizing over the hidden vectors can be carried out analytically. This allows dfor an equivalent definition of the RBM model in terms of the <strong>free energy</strong> <span class="math">\(F(x)\)</span>:</p>
<div class="math">$$
P_{\theta}(x) = \frac{1}{Z} \exp\left( -F_{\theta}(x) \right),
Z = \sum\limits_{x'\in\mathcal{X}} \exp\left( -F_{\theta}(x) \right),
F_{\theta}(x) = -\left[ x^Tb + \sum\limits_{k=1}^{K} \log\left( 1+\exp(x^TW_k+c_k) \right) \right]
$$</div>
<p>Note that the partition function still involves marginalizing over all <span class="math">\(2^D\)</span> elements of the set <span class="math">\(\mathcal{X}\)</span>, rendering the computation of the partition function intractable for even moderate <span class="math">\(D\)</span>.</p>
<p>The first view is useful for deriving stochastic approximation methods based on alternating sampling of the visible and hidden vectors. The second view is useful for applying inductive principles that can not naturally deal with the presence of latent variables.</p>
<p>RBM represents an ansatz for a probability distribution. The goal of training is to fit the parameters of this distribution such that it resembles the distribution of the training data.
The training data is seen as a sample of the unknown target distribution <span class="math">\(q\)</span>.</p>
<p>Basic scheme:</p>
<ul>
<li>BMs with restricted network topology: units organized in two layers (hidden and visible), no connections within the layers</li>
<li>visible units correspond to the components of an observation</li>
<li>hidden units model dependencies between the components of observations, can be viewed as non-lin feature detectors</li>
<li>parametrized generative models representing a PD</li>
<li>It is "restricted" because no intra-layer connections.</li>
</ul>
<p>Energy function:</p>
<div class="math">\begin{aligned}
E_{\theta}(v,h) &amp;=  -v^T W h -v^T a - h^T b \\
                &amp;= - \sum\limits_{i=1}^V a_i v_i - \sum\limits_{j=1}^H b_j h_j - \sum\limits_{ij} v_i W_{ij} h_j
\end{aligned}</div>
<p>where <span class="math">\(v_i, h_j\)</span> are binary states of visible unit <span class="math">\(i\)</span> and hidden unit <span class="math">\(j\)</span>, that is, <span class="math">\(v_i\in\{0,1\}^V, h_j\in\{0,1\}^H\)</span>.</p>
<p><span class="math">\(a_i\)</span>, <span class="math">\(b_j\)</span> are their respective biases. <span class="math">\(W_{ij}\)</span> is the symmetric connection weight between the units.</p>
<p>The network assigns a probability to every possible pair of visible and hidden vector via</p>
<p><span class="math">\(p(v,h) = \frac{1}{Z} e^{-E(v,h)}\)</span>,</p>
<p>where the partition function <span class="math">\(Z\)</span> is given by summing over all possible pairs of visible and hidden vectors:</p>
<p><span class="math">\(Z = \sum\limits_{v,h} e^{-E(v,h)}\)</span>.</p>
<p>The probability that the network assigns to a visible vector, <span class="math">\(v\)</span>, is given by summing over all possible hidden vectors:</p>
<p>$
p(v) = \frac{1}{Z} \sum\limits_{h} e^{-E(v,h)}.
$</p>
<p>The lower the energy of an input <span class="math">\(v\)</span>, the higher its assigned probability.</p>
<p>The derivative of the <span class="math">\(\log\)</span> probability of a visible vector with respect to a weight is simple:</p>
<div class="math">$$
    \frac{\partial \log\[p(v)\]}{\partial W_{ij}} = \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{model}
$$</div>
<p>where the angle brackets denote expectation values under the distributions specified by the subscript.</p>
<p>My derivation of this result:</p>
<p>\usepackage{physics}
\begin{document}
[
\dv{Q}{t} = \dv{s}{t}  \quad
\dv[n]{Q}{t} = \dv[n]{s}{t}  \quad
\pdv{Q}{t} = \pdv{s}{t}  \quad
\pdv[n]{Q}{t} = \pdv[n]{s}{t}  \quad
\pdv{Q}{x}{t} = \pdv{s}{x}{t}  \quad
]
[
\fdv{F}{g}
]</p>
<p>\begin{align}</p>
<div class="highlight"><pre><span></span>\pdv{ \log\[p(v)\] }{ W_{ij} } &amp;= \pdv{ }{ W_{ij} } \log\left[ \frac{1}{Z} \sum\limits_{h} e^{-E(v,h)} \right]\\
&amp;= \pdv{ }{ W_{ij} } \left{ \log \left[ \sum\limits_{h} e^{-E(v,h)} \right] - \log \left[ Z \right] \right} \\
&amp;= \left[ \sum\limits_{h} e^{-E(v,h)} \right]^{-1} \pdv{ }{ W_{ij} } \sum\limits_{h} e^{-E(v,h)} - 
   Z^{-1} \pdv{ }{ W_{ij} } Z \\
&amp;= \left[ Z p(v) \right]^{-1} \pdv{ }{ W_{ij} } \sum\limits_{h} e^{-E(v,h)} - Z^{-1} \pdv{ }{ W_{ij} } Z \\
&amp;= \left[ Z p(v) \right]^{-1} \sum\limits_{h} v_i h_j e^{-E(v,h)} - Z^{-1} \sum\limits_{v,h} v_i h_j e^{-E(v,h)} \\
&amp;= \sum\limits_{h} v_i h_j \frac{p(v,h)}{p(v)} - \sum\limits_{v,h} v_i h_j p(v,h) \\
&amp;= \sum\limits_{h} v_i h_j p(h|v) - \sum\limits_{v,h} v_i h_j p(v,h)
</pre></div>


<p>\end{align}</p>
<div class="math">\begin{align}
    \pdv{ }{ W_{ij} } \sum\limits_{h} e^{-E(v,h)} &amp;= \pdv{ }{ W_{ij} } \sum\limits_{h} \exp\left[ \sum\limits_{k} a_k v_k + \sum\limits_{l} b_l h_l + \sum\limits_{kl} v_k W_{kl} h_l \right] \\
    &amp;= e^{a^Tv} \sum\limits_{h} e^{b^Th} \pdv{ \exp\left[ \sum\limits_{l} b_l h_l \right] }{ W_{ij} } \\
    &amp;= e^{a^Tv} \sum\limits_{h} e^{b^Th} v_i h_j e^{v^T W h} \\
    &amp;= \sum\limits_{h} v_i h_j e^{-E(v,h)}
\end{align}</div>
<div class="math">\begin{align}
    \pdv{ }{ W_{ij} } Z &amp;= \pdv{ }{ W_{ij} } \sum\limits_{v,h} e^{-E(v,h)} \\
                        &amp;= \sum\limits_{v} \pdv{ }{ W_{ij} } \sum\limits_{h} e^{-E(v,h)} \\
                        &amp;= \sum\limits_{v,h} v_i h_j e^{-E(v,h)}
\end{align}</div>
<p>This leads to a very simple learning rule for performing stochastic steepest ascent on the log probability of the training data:</p>
<div class="math">$$
    \Delta W_{ij} = \eta \left( \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{model} \right)
$$</div>
<p>where <span class="math">\(\eta\)</span> is the learning rate.</p>
<p>Because there are no direct connections between hidden units in an RBM, it is very easy to get an unbiased sample of <span class="math">\(\langle v_i h_j \rangle_{data}\)</span>. GIven a randomly selected training image v, the binary state <span class="math">\(h_j\)</span> of each hidden unit <span class="math">\(j\)</span> is 1 with probability</p>
<div class="math">$$
    p(h_j=1 | v) = \sigma\left( b_j + \sum_i v_i W_{ij} \right) (eq. 7)
$$</div>
<p>where <span class="math">\(\sigma\)</span> is the logistic function. <span class="math">\(v_i h_j\)</span> is then an unbiased sample.</p>
<p>Similarly, because there are no connections between visible units in an RMB, it is also very is easy to get an unboased sample of the state of a visible unit given a hidden vector <span class="math">\(h\)</span>:</p>
<div class="math">$$
    p(v_i=1 | h) = \sigma\left( a_i + \sum_j h_j W_{ij} \right). (eq. 8)
$$</div>
<p>Getting an unbiased sample of <span class="math">\(\langle v_i h_j \rangle_{model}\)</span>, however, is much more difficult. Can be done by starting at any random state of the visible units and performing alternating Gibbs sampling for a very long time. One iteration of alternating Gibbs sampling consists of updating all of the hidden units in parallel using eq. 7, followed by updating all of the visible units in parallel using eq. 8.</p>
<p>Much faster learning procedure proposed by Hinton in 2002:
- start by setting the visibles to a training vector
- then the states of the hiddens are computed in parallel using eq. 7
- once binary states of the hiddens have been chosen, a "reconstruction" is produced by setting each visible to 1 with a probability given by eq. 8
- the change in a weight is then given by</p>
<div class="math">$$
    \Delta W_{ij} = \eta \left( \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{reconstr} \right)
$$</div>
<p>A simplified version of the same learning rule is applied to the biases, it uses the states of individual units instead of pairwise products.</p>
<p>The learning works well even though it is only crudely approximating the gradient of the log probability of the training data. It is much more closely approximating the gradient of another objective function, called the Contrastive Divergence. The CD is the difference between two KL divergences, but the described procedure ignores one tricky term in this objective function, so it is not even following that gradient. Sutskever and Tieleman have shown that it does not follow the gradient of any function. Nevertheless, it works well enough in many significant applications.</p>
<p>RBM typically learn better models if more steps of alternating Gibbs sampling are used before collecting the statistics for the second term in the learning rule, which will be called the negative statistics. <span class="math">\(CD_n\)</span> will refer to denote learning using <span class="math">\(n\)</span> full stapes of alternating Gibbs sampling.</p>
<h2>3. How to collect statistics when using CD</h2>
<p>Assume:
- hidden and visible units are binary
- the purpose of learning is to create a good generative model of the set of training vectors</p>
<p>Updating the hidden states:
Assume using CD<span class="math">\(_1\)</span>; the hiddens should have binary states when they are being driven by a data vector.
The probability of turning on a hidden unit j is computed by applying the logistic function to its total input</p>
<div class="math">$$
    p(h_j=1) = \sigma\left( b_j+\sum\limits_i v_i W_{ij} \right)
$$</div>
<p>
and the hidden unit turns on if this probability is greater than a random number uniformly distributed between 0 and 1.</p>
<p>It is very important to make these hidden states binary rather than using the probabilities themselves!</p>
<p>For the last update of the hiddens, however, use the probabilities itself to avoid unnecessary sampling noise.
When using CD<span class="math">\(_n\)</span>, only the final update of the hiddens should use the probability.</p>
<p>Updating the visible states:
When generating a reconstruction, stochastically pick 0 or 1 with probability determined by the total top-down input:</p>
<div class="math">$$
    p_i = p(v_i=1) = \sigma\left( a_i + \sum\limits_j h_jW_{ij} \right)
$$</div>
<p>However, it is common to use the probability itself instead of sampling a binary value, since it reduces sampling noise and thus allows faster learning. This is not as problematic as in case of the hiddens.</p>
<p>Collecting the statistics needed for learning</p>
<p>SAsuming that the visibles are using real-valued probabilities instead of binary values, there are two sensible ways to collect the positive statistics for the connection between visible unit i and hidden unit j:</p>
<p><span class="math">\(\langle p_i h_j \rangle_{data}\)</span> or <span class="math">\(\langle p_i p_j \rangle_{data}\)</span></p>
<p>where <span class="math">\(h_j\)</span> is a binary state that takes value 1 with probability <span class="math">\(p_j\)</span>.
Using <span class="math">\(h_j\)</span> is closer to the mathematical model of an RBM, but using <span class="math">\(p_j\)</span> usually has less sampling noise which allows slightly faster learning.</p>
<p>Getting a learning signal for CD<span class="math">\(_1\)</span>:
When the hidden units are being driven by data, use stochastic binary states only.
When the hidden units are being driven by data, use probabilities without sampling.</p>
<p>Assuming the visibles use the logistic function, use probabilities for both the data and the reconstruction.</p>
<p>When collecting pairwise statistics for learning weights or the individual statistics for learning biases, use the probabilities, not the binary states, and make sure the weights have random initial values to break symmetry.</p>
<h2>from: Fischer, Igel</h2>
<h2>1. Introduction</h2>
<p>Boltzmann Machines (BMs):</p>
<div class="highlight"><pre><span></span>- bidirectionally connected networks of stochastic processing units
- or undirected graphical models, also known as Markov random fields
- can learn important aspects of an unknown probability distribution (PD) based on samples from this distribution
- learning = adjusting the parameters s.t. the PD represented by the BM fits the training data as well as possible
- however, the process is difficult and time-consuming
</pre></div>


<p>Restricted BMs:</p>
<div class="highlight"><pre><span></span>- BMs with restricted network topology: units organized in two layers (hidden and visible), no connections within the layers
- visible units correspond to the components of an observation
- hidden units model dependencies between the components of observations, can be viewed as non-lin feature detectors
- parametrized generative models representing a PD
- learning is simpler
- after successful learning, an RBM provides a closed-form representation of the distribution underlying the observations
</pre></div>


<p>Computational difficulties:
    - computing the likelihood of an undirected model or its gradient for inference is i.g. computationally intensive, 
    also for RBMs
    - therefore, sampling-based methods are employed to approximate the likelihood and its gradient
    - samploing from an undirected graphical model is i.g. not straightforward, but for RBMs Markov Chain Monte Carlo (MCMC) methods are easily applicable in the form of Gibbs sampling</p>
<ul>
<li>single as well as stacked RBMs can be reinterpreted as deterministic feed-forward NNs</li>
<li>the NN corresponding to a trained RBM or DBB can be augmented by an output layer, where units in the new added output layer represent labels corresponding to observations; then the model corresponds to a standard neural network for classification or regression that can be further trained by standard supervised learning algos</li>
<li>this initialization (unsupervised pre-training) of the feed-forward NN's weights based on a generative model helps</li>
</ul>
<h2>2. Graphical Models</h2>
<p>Zweck of graphical models:</p>
<ul>
<li>
<p>Probabilistic graphical models describe probability distributions (PDs) 
  by mapping conditional (in)dependence properties between random variables 
  on a graph structure.</p>
</li>
<li>
<p>Two sets of RVs X_1, X_2 are cond indep given a set of RVs X_3 if
  p(X_1, X_2 | X_3) = p(X_1|X_3)p(X_2|X_3).</p>
</li>
<li>
<p>Visualization by graphs is helpful for</p>
<ul>
<li>understanding and motivating probab models</li>
<li>deriving complex computations by using algos exploiting the graph structure</li>
</ul>
</li>
</ul>
<h2>2.1 Undirected Graphs and Markov Random Fields</h2>
<h2>Fundamental concepts of Graph Theory</h2>
<ul>
<li>an undirected graph is a tuple G=(V,E), where V is a finite set of nodes and E is a set of undirected edges</li>
<li>an edge consists of a pair of nodes from V</li>
<li>if there exists an edge between two nodes v and w, i.e. {v,w} \elem E, w belongs to the neighborhood of v and v.v.</li>
<li>the neighborhood \mathcal{N}_v = {w\in V: {w,v}\in E} of v is defined by the dset of nodes connected to v</li>
<li>a clique is a subset of V in which all nodes are pairwise connected</li>
<li>a clique is maximal if no node can be added such that the resulting set is still a clique</li>
<li>denote by C the set of all maximal cliques of an undirected graph</li>
<li>a path from v_1 to v_m is a sequence of nodes v_1, v_2, ..., v_m \in V with {v_i,v_{i+1}}\in E for i=1,...,m-1</li>
<li>
<p>a set \mathcal{V} \sub V separates two nodes v\notin \mathcal{V} and w\notin \mathcal{V}, if every path from v to w contains a node from \mathcal{V}</p>
</li>
<li>
<p>associate a random variable (RV) X_v, taking values in a state space \Lambda_v, with each node v in an undirected graph G=(V,E)</p>
</li>
<li>for slimmer notation, assume \Lambda_v = \Lambda for all v\in V</li>
<li>the RVs \bold{X}=(X_v)_{v\in V} are called Markov random field (MRF) if
  the joint PD p fulfills the (global) Markov property wrt the graph:</li>
</ul>
<p>For all disjunct subsets \mathcal{A}, \mathcal{B}, \mathcal{S} \subset V, 
  where all nodes in \mathcal{A} and \mathcal{B} are separated by \mathcal{S},
  the variables (X_a)<em _mathcal_B="\mathcal{B" b_in="b\in">{a\in \mathcal{A}} and (X_b)</em>} 
  are conditional independent given (X_s)<em a_in_mathcal_A="a\in\mathcal{A">{s\in \mathcal{S}}, i.e. 
  for all \bold{x}\in\Lambda^{|V|} it holds
  p((x_a)</em>}|(x_t)<em a_in_mathcal_A="a\in\mathcal{A">{t\in\mathcal{S}\cup \mathcal{B}}) = p((x_a)</em>}|(x_t)_{t\in\mathcal{S})</p>
<ul>
<li>a set of nodes MB(v) is called the Markov blanket of node v 
  if for any set of nodes \mathcal{B} with v\notin\mathcal{B} we have 
  p(v|MB(v),\mathcal{B}) = p(v|MB(v))
  this means that v is conditional independent from any other variables given MB(v)</li>
<li>in a MRF, the Markov blanket MB(v) is given by the neighborhood \mathcal{N}_v of v,
  a fact that is also referred to as local Markov property</li>
</ul>
<p>--&gt; conditional independence of RVs and the factorization properties of the joint PD are closely related
The Hammersley-Clifford Theorem states that a strictly positive distribution p satisfies the Markov property wrt an undirected graph G iff p factorizes over G.
A distribution is said to factorize over an undirected graph G with maximal cliques C if there exists a set of non-negative functions {\psi_C}_{C\subset \mathcal{C}}, called potential functions, with</p>
<p>\forall \bold{x, \hat{x}} \in \Lambda^{|V|}: (x_c)<em C c_in="c\in">{c\in C} = (\hat{x}_c)</em> \Rightarrow \psi_C(\bold{x}) = \psi_C(\hat{\bold{x}})
  and
  p(x) = \frac{1}{Z} \Prod \limits_{C\in \mathcal{C}} \psi_C(\bold{x})</p>
<p>The normalization constant Z = \sum_{\bold{x}} \Prod_{C\in \mathcal{C}} \psi_C(\bold{x}_C) is called partition function.</p>
<p>If p is strictly positive, the same holds for the potential functions. Thus we can write</p>
<p>p(\bold{x}) = \frac{1}{Z} \Prod \limits_{C\in \mathcal{C}} \psi_C(\bold{x}_C) = ... = \frac{1}{Z} e^{-E(\mathcal{x})}</p>
<p>where E=\sum_{C\in \mathcal{C}} \ln\psi_C(\bold{x}_C) we call the energy function.</p>
<p>Thus, due to the Hammersley-Clifford-Theorem:</p>
<p>The PD of every MRF can be expressed as a Gibbs distribution. </p>
<h2>2.2 Unsupervised Learning</h2>
<p>means learning (important aspects of) an unknown distribution <span class="math">\(q\)</span> based on sample data.
If the structure of the graphical model is known and the energy function belongs to a known family of functions parameterized by <span class="math">\(\theta\)</span>, then unsupervised learning of a data distribution with an MRF means adjusting the parameters <span class="math">\(\theta\)</span>.</p>
<p>Consider training data <span class="math">\(S=\{x_1, ..., x_l\}\)</span>, with data samples being i.i.d., i.e., independently drawn from some unknown distribution <span class="math">\(q\)</span>.
Standard way of estimating the parameters of a statistical model is maximum-likelihood estimation. For MRF, this corresponds to finding the parameters that maximize the probability of <span class="math">\(S\)</span> under the MRF distribution; i.e., training goal is finding the parameters <span class="math">\(\theta\)</span> that maximize the likelihood given the training data.</p>
<p>The likelihood of an MRF maps parameters <span class="math">\(\theta\)</span> from the parameter space <span class="math">\(\Theta\)</span> to </p>
<div class="math">$$
    \mathcal{L}(\theta|S) = \prod\limits_{i=1}^l p(x_i|\theta).
$$</div>
<p>Maximizing the likelihood is the same as maximizing the log-likelihood given by:</p>
<div class="math">$$
    \ln \mathcal{L}(\theta|S) = \ln \prod\limits_{i=1}^l p(x_i|\theta) = \sum\limits_{i=1}^l \ln p(x_i|\theta).
$$</div>
<p>For the Gibbs distribution of an MRF it is in general not possible to find the maximum likelihood parameters analytically, and therefore numerical approximation methids are used, such as gradient ascent.</p>
<p>Maximizing the likelohood is equivalent to minimizing the distance between the unknown distribution <span class="math">\(q\)</span> underlying <span class="math">\(S\)</span> and the distribution <span class="math">\(p\)</span> of the MRF in terms of the KL-divergence, which for a finite state space <span class="math">\(\Omega\)</span> is given by:</p>
<div class="math">$$
    KL(q||p) = \sum\limits_{x\in\Omega} q(x)\ln\frac{q(x)}{p(x)}
$$</div>
<p>non-symmetric measure</p>
<p>can be written as difference between entropy of <span class="math">\(q\)</span> and a second term, only the latter depends on the parameters subject to optimization</p>
<p>Approximating the expectation over <span class="math">\(q\)</span> in this term by the training samples from <span class="math">\(q\)</span> results in 
the log-likelihood. Therefore, maximizing the log-likelihood corresponds to minimizing KL-divergence.</p>
<h2>Optimization by Gradient Ascent</h2>
<p>If it is not possiblet to find parameters maximizing the log-likelihood analytically, the usual way to find them is gradient ascent on the log-likelihood. Update rule:</p>
<h2>3. Markov Chains and Markov Chain Monte Carlo Techniques</h2>
<p>Markov Chains provide a methods to draw samples from probability distributions like the Gibbs distribution of an MRF. Gibbs sampling is a technique often used for MRF training and in particular for training RBMs.</p>
<p>3.1 Markov Chain and Convergence to Stationarity</p>
<p>A Markov Chain is a time descrete stochastic process for which the Markov property holds. That is, a family of random variables <span class="math">\(X=\{ X^{(k)} | k\in \mathbb{N}_0 \}\)</span> which take valus in a (finite) set <span class="math">\(\Omega\)</span>, an for which <span class="math">\(\forall k \geq 0\)</span> and <span class="math">\(\forall j, i, i_0, ..., i_{k-1} \in \Omega\)</span> it holds</p>
<div class="math">$$
    p_{ij}^k = ...
$$</div>
<p>i.e., the next state of the system depends only on the current state and not on the sequence of states that preceded it.</p>
<p>The chain is called homogeneous if <span class="math">\(\forall k\geq 0, p_{ij}^k = p_ij\)</span>, and the matrix <span class="math">\(\mathbb{P} = (p_{ij})_{i,j\in\Omega}\)</span> is called the transition matrix.</p>
<p>The PD <span class="math">\(\mu^{(k)}\)</span> of <span class="math">\(X^{(k)}\)</span> is given by <span class="math">\(\mu^{(k)T} = \mu^{(0)T}\mathbb{P}\)</span>, where <span class="math">\(\mu^{0}\)</span> is the PD of <span class="math">\(X^{(0)}\)</span>, given by the probability vector <span class="math">\(\mu^{0} = (\mu^{0}(i))_{i\in\Omega}\)</span> with <span class="math">\(\mu^{0}(i) = P(X^{(0)}=i)\)</span>.</p>
<p>A PD <span class="math">\(\pi\)</span> for which it holds <span class="math">\(\pi^T = \pi^T \mathbb{P}\)</span> is called stationary.
If the Markov chain for any time <span class="math">\(k\)</span> reached the stationary distribution <span class="math">\(\mu^{(k)}=\pi\)</span>, all subsequent states will be distributed accordingly, i.e. <span class="math">\(\mu^{(k+n)}=\pi \forall n \in \mathbb{N}\)</span>.</p>
<p>A sufficient (but not necessary) condition for a distribution <span class="math">\(\pi\)</span> to be stationary w.r.t. a Markov chain described by the transition probabilities <span class="math">\(p_{ij}, i, j \in \Omega\)</span> is that the <strong>detailed balance</strong></p>
<div class="math">$$
    \pi(i) p_{ij} = \pi(j) p_{ji}
$$</div>
<p><span class="math">\(\forall i,j \in \Omega\)</span>, holds.</p>
<p>Especially relevant are Markov chains for which it is known that there exists a unique stationary distribution.</p>
<p>For finite <span class="math">\(\Omega\)</span>, this is the case if the Markov chain is <strong>irreducible</strong>.</p>
<p>A Markov chain is irreducible if one can get from any state in <span class="math">\(\Omega\)</span> to any other ina finite number of transitions. More formally, if <span class="math">\(\forall i,j \in \Omega \exists k\gt 0\)</span> with <span class="math">\(P(X^{(k)}=j | X^{(0)}=i) \gt 0\)</span>.</p>
<p>A Markov chain is aperiodic if <span class="math">\(\forall i \in \Omega\)</span> the greatest common divisor of <span class="math">\(\{k|P(X^{(k)}=i|X^{(0)}=i)\gt 0 \hat k\in\mathbb{N}_0\}\)</span> is 1.</p>
<p>An irreducible and aperiodic Markov chain ona finite state space is guaranteed to converge to its stationary distribution. That is, for an arbitrary starting distribution <span class="math">\(\mu\)</span> it holds:</p>
<div class="math">$$
    \lim_{k\to\inf} d_V(\mu^TP^k, \pi^T) = 0,
$$</div>
<p>where <span class="math">\(d_V\)</span> is the <strong>distance of variation</strong>, defined as</p>
<div class="math">$$
    d_V(\alpha, \beta) = \frac{1}{2}|\alpha-\beta| = \frac{1}{2}\sum\limits_{x\in\Onega}|\alpha(x)-\beta(x)|
$$</div>
<p>for two distributions <span class="math">\(\alpha, \beta\)</span> on a finite state space <span class="math">\(\Omega\)</span>.</p>
<p>MCMC methods make use of the convergence theorem for producing samples from certain distributions by setting up a Markov chain that converges to the desired distributions:</p>
<p>Suppose you want to sample from a finite distribution <span class="math">\(q\)</span>. You construct an irreducible and aperiodic Markov chain with stationary distribution <span class="math">\(\pi = q\)</span> - which is a nontrivial task. If <span class="math">\(t\)</span> is large enough, a sample <span class="math">\(X^{(t)}\)</span> from the constructed chain is then approximately a sample from <span class="math">\(\pi\)</span> and therefore form <span class="math">\(q\)</span>. Gibbs sampling is such a MCMC method.</p>
<h2>3.2 Gibbs Sampling</h2>
<p>belongs to the class of Metropolis-Hastings algorithms.</p>
<p>a simple MCMC algo for producing samples from the joint PD of multiple random variables.</p>
<p>basic idea: update each variable subsequently based on its conditional distribution given the satte of the others.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></p>
                <br />
                <br />
                <p class="tags tagcloud">
                     <a href="../tag/machine-learning/">machine learning</a>
                    <a href="../tag/unsupervised-learning/">unsupervised learning</a>
                    <a href="../tag/restricted-boltzmann-machines/">Restricted Boltzmann Machines</a>
                    <a href="../tag/rbm/">RBM</a>
                    <a href="../tag/tutorial/">tutorial</a>
                 </p>
            </section>
        </div>

    </section>

    <!-- Two 
    <section class="wrapper style1 container special">
        <div class="row">
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/machine-learning-for-quantum-design-at-pi.html" rel='bookmark'><h3>Machine Learning for Quantum Design at PI</h3></a>
              </header>
              <p>The Perimeter Institute for Theoretical Physics in Waterloo, Canada, hosts "</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/machine-learning-for-quantum-design-at-pi.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/workshop-machine-learning-for-quantum-technology-at-mpl-erlangen.html" rel='bookmark'><h3>Workshop Machine Learning for Quantum Technology at MPL Erlangen</h3></a>
              </header>
              <p>The Max Planck Institute for the Science of Light in Erlangen, Germany, hosts <a href="https://www.mpl.mpg.de/divisions/marquardt-division/workshops/2019-machi</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/workshop-machine-learning-for-quantum-technology-at-mpl-erlangen.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
          <div class="4u">

            <section>
              <span class="icon feature fa-"></span>
              <header>
                <a href="../blog/DL-theory.html" rel='bookmark'><h3>The Theory of Deep Learning - Part I</h3></a>
              </header>
              <p>Why do modern deep neural networks (DNNs) perform so well on previously unseen test data, even when their number of weights is much larger than the number of data points</p>
              <footer>
                  <ul class="buttons">
                      <li><a href="../blog/DL-theory.html" class="button small">Read More</a></li>
                  </ul>
              </footer>
            </section>

          </div>
        </div>
    </section> -->

</article>

 
<!-- Footer -->
<footer id="footer">

    <ul class="icons">
    </ul>

    <span class="copyright">&copy; physicsml. All rights reserved. <br> Powered by <a href="https://blog.getpelican.com">Pelican</a>. Theme Twenty, Design: <a href="http://html5up.net">HTML5 UP</a>. Implemented and maintained by Anna Go.</span>

</footer>
</body>
</html>